{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOc4PyyB/Jv1P3GzU4hVRhv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gjduart/Graph-Learning/blob/main/GIN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os"
      ],
      "metadata": {
        "id": "vL9NMOU1TpYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Torch Geometric"
      ],
      "metadata": {
        "id": "58qbts-dT17W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7UJskU0TYMh",
        "outputId": "91f46a02-3e05-42d5-edc5-7003584ae6db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.12.1+cu113\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 2.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 2.9 MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Dataset"
      ],
      "metadata": {
        "id": "hYYeol75Vc-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.datasets import TUDataset\n",
        "\n",
        "dataset = TUDataset(root='.', name='PROTEINS').shuffle()\n",
        "\n",
        "print(f'Informations of Dataset: {dataset}')\n",
        "print('-------------------')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of nodes: {dataset[0].x.shape[0]}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKvfmlfBVbLD",
        "outputId": "76683d75-7667-4005-fcf5-cd15e9a24946"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Informations of Dataset: PROTEINS(1113)\n",
            "-------------------\n",
            "Number of graphs: 1113\n",
            "Number of nodes: 23\n",
            "Number of features: 3\n",
            "Number of classes: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xtED-9q_XPhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_kUBO5hS4FI",
        "outputId": "cbb495a5-1225-4c27-e4d8-75b25075007f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set   = 890 graphs\n",
            "Validation set = 111 graphs\n",
            "Test set       = 112 graphs\n",
            "\n",
            "Train loader:\n",
            " - Subgraph 0: DataBatch(edge_index=[2, 11058], x=[3019, 3], y=[64], batch=[3019], ptr=[65])\n",
            " - Subgraph 1: DataBatch(edge_index=[2, 9442], x=[2568, 3], y=[64], batch=[2568], ptr=[65])\n",
            " - Subgraph 2: DataBatch(edge_index=[2, 9766], x=[2684, 3], y=[64], batch=[2684], ptr=[65])\n",
            " - Subgraph 3: DataBatch(edge_index=[2, 9990], x=[2732, 3], y=[64], batch=[2732], ptr=[65])\n",
            " - Subgraph 4: DataBatch(edge_index=[2, 10120], x=[2676, 3], y=[64], batch=[2676], ptr=[65])\n",
            " - Subgraph 5: DataBatch(edge_index=[2, 9786], x=[2604, 3], y=[64], batch=[2604], ptr=[65])\n",
            " - Subgraph 6: DataBatch(edge_index=[2, 7538], x=[1962, 3], y=[64], batch=[1962], ptr=[65])\n",
            " - Subgraph 7: DataBatch(edge_index=[2, 8896], x=[2350, 3], y=[64], batch=[2350], ptr=[65])\n",
            " - Subgraph 8: DataBatch(edge_index=[2, 9722], x=[2704, 3], y=[64], batch=[2704], ptr=[65])\n",
            " - Subgraph 9: DataBatch(edge_index=[2, 9236], x=[2540, 3], y=[64], batch=[2540], ptr=[65])\n",
            " - Subgraph 10: DataBatch(edge_index=[2, 8034], x=[2063, 3], y=[64], batch=[2063], ptr=[65])\n",
            " - Subgraph 11: DataBatch(edge_index=[2, 8994], x=[2339, 3], y=[64], batch=[2339], ptr=[65])\n",
            " - Subgraph 12: DataBatch(edge_index=[2, 7946], x=[2083, 3], y=[64], batch=[2083], ptr=[65])\n",
            " - Subgraph 13: DataBatch(edge_index=[2, 6714], x=[1751, 3], y=[58], batch=[1751], ptr=[59])\n",
            "\n",
            "Validation loader:\n",
            " - Subgraph 0: DataBatch(edge_index=[2, 8916], x=[2442, 3], y=[64], batch=[2442], ptr=[65])\n",
            " - Subgraph 1: DataBatch(edge_index=[2, 8068], x=[2245, 3], y=[47], batch=[2245], ptr=[48])\n",
            "\n",
            "Test loader:\n",
            " - Subgraph 0: DataBatch(edge_index=[2, 11462], x=[3007, 3], y=[64], batch=[3007], ptr=[65])\n",
            " - Subgraph 1: DataBatch(edge_index=[2, 6400], x=[1702, 3], y=[48], batch=[1702], ptr=[49])\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "# Create training, validation, and test sets\n",
        "train_dataset = dataset[:int(len(dataset)*0.8)]\n",
        "val_dataset   = dataset[int(len(dataset)*0.8):int(len(dataset)*0.9)]\n",
        "test_dataset  = dataset[int(len(dataset)*0.9):]\n",
        "\n",
        "print(f'Training set   = {len(train_dataset)} graphs')\n",
        "print(f'Validation set = {len(val_dataset)} graphs')\n",
        "print(f'Test set       = {len(test_dataset)} graphs')\n",
        "\n",
        "# Create mini-batches\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "print('\\nTrain loader:')\n",
        "for i, subgraph in enumerate(train_loader):\n",
        "    print(f' - Subgraph {i}: {subgraph}')\n",
        "\n",
        "print('\\nValidation loader:')\n",
        "for i, subgraph in enumerate(val_loader):\n",
        "    print(f' - Subgraph {i}: {subgraph}')\n",
        "\n",
        "print('\\nTest loader:')\n",
        "for i, subgraph in enumerate(test_loader):\n",
        "    print(f' - Subgraph {i}: {subgraph}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Class of GIN"
      ],
      "metadata": {
        "id": "YdOnzcsqUOyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import GINConv\n",
        "from torch.nn import Linear, Sequential, BatchNorm1d, ReLU\n",
        "from torch_geometric.nn import GCNConv, GINConv\n",
        "from torch_geometric.nn import global_mean_pool, global_add_pool\n",
        "import torch.nn.functional as F\n"
      ],
      "metadata": {
        "id": "dzfoyjdCUUSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sVFo5_HeWDPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9frftpyFTJGm"
      },
      "outputs": [],
      "source": [
        "from torch.nn import Linear, Sequential, BatchNorm1d, ReLU, Dropout\n",
        "\n",
        "\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    \"\"\"GCN\"\"\"\n",
        "    def __init__(self, dim_h):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(dataset.num_node_features, dim_h)\n",
        "        self.conv2 = GCNConv(dim_h, dim_h)\n",
        "        self.conv3 = GCNConv(dim_h, dim_h)\n",
        "        self.lin = Linear(dim_h, dataset.num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        # Node embeddings \n",
        "        h = self.conv1(x, edge_index)\n",
        "        h = h.relu()\n",
        "        h = self.conv2(h, edge_index)\n",
        "        h = h.relu()\n",
        "        h = self.conv3(h, edge_index)\n",
        "\n",
        "        # Graph-level readout\n",
        "        hG = global_mean_pool(h, batch)\n",
        "\n",
        "        # Classifier\n",
        "        h = F.dropout(hG, p=0.5, training=self.training)\n",
        "        h = self.lin(h)\n",
        "        \n",
        "        return hG, F.log_softmax(h, dim=1)\n",
        "\n",
        "gcn = GCN(dim_h=32)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GIN(torch.nn.Module):\n",
        "    \"\"\"GIN\"\"\"\n",
        "    def __init__(self, dim_h):\n",
        "        super(GIN, self).__init__()\n",
        "        self.conv1 = GINConv(\n",
        "            Sequential(Linear(dataset.num_node_features, dim_h),\n",
        "                       BatchNorm1d(dim_h), ReLU(),\n",
        "                       Linear(dim_h, dim_h), ReLU()))\n",
        "        self.conv2 = GINConv(\n",
        "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
        "                       Linear(dim_h, dim_h), ReLU()))\n",
        "        self.conv3 = GINConv(\n",
        "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
        "                       Linear(dim_h, dim_h), ReLU()))\n",
        "        self.lin1 = Linear(dim_h*3, dim_h*3)\n",
        "        self.lin2 = Linear(dim_h*3, dataset.num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        # Node embeddings \n",
        "        h1 = self.conv1(x, edge_index)\n",
        "        h2 = self.conv2(h1, edge_index)\n",
        "        h3 = self.conv3(h2, edge_index)\n",
        "\n",
        "        # Graph-level readout\n",
        "        h1 = global_add_pool(h1, batch)\n",
        "        h2 = global_add_pool(h2, batch)\n",
        "        h3 = global_add_pool(h3, batch)\n",
        "\n",
        "        # Concatenate graph embeddings\n",
        "        h = torch.cat((h1, h2, h3), dim=1)\n",
        "\n",
        "        # Classifier\n",
        "        h = self.lin1(h)\n",
        "        h = h.relu()\n",
        "        h = F.dropout(h, p=0.5, training=self.training)\n",
        "        h = self.lin2(h)\n",
        "        \n",
        "        return h, F.log_softmax(h, dim=1)\n",
        "\n",
        "gin = GIN(dim_h=32)"
      ],
      "metadata": {
        "id": "ggZpgQxGTgFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g-jy0GxPWv-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YsYYubhTE22",
        "outputId": "24494779-4c1a-4b05-d0b9-eedb07bba9a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100 | Train Loss: 0.68 | Train Acc: 59.24% | Val Loss: 0.66 | Val Acc: 64.23%\n",
            "Test Loss: 0.68 | Test Acc: 57.03%\n",
            "Epoch 100 | Train Loss: 0.51 | Train Acc: 77.15% | Val Loss: 0.54 | Val Acc: 73.60%\n",
            "Test Loss: 0.57 | Test Acc: 74.48%\n"
          ]
        }
      ],
      "source": [
        "def train(model, loader):\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(),\n",
        "                                      lr=0.01,\n",
        "                                      weight_decay=0.01)\n",
        "    epochs = 100\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs+1):\n",
        "      total_loss = 0\n",
        "      acc = 0\n",
        "      val_loss = 0\n",
        "      val_acc = 0\n",
        "\n",
        "      # Train on batches\n",
        "      for data in loader:\n",
        "        optimizer.zero_grad()\n",
        "        _, out = model(data.x, data.edge_index, data.batch)\n",
        "        loss = criterion(out, data.y)\n",
        "        total_loss += loss / len(loader)\n",
        "        acc += accuracy(out.argmax(dim=1), data.y) / len(loader)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Validation\n",
        "        val_loss, val_acc = test(model, val_loader)\n",
        "\n",
        "    # Print metrics every 10 epochs\n",
        "    if(epoch % 10 == 0):\n",
        "        print(f'Epoch {epoch:>3} | Train Loss: {total_loss:.2f} '\n",
        "              f'| Train Acc: {acc*100:>5.2f}% '\n",
        "              f'| Val Loss: {val_loss:.2f} '\n",
        "              f'| Val Acc: {val_acc*100:.2f}%')\n",
        "          \n",
        "    test_loss, test_acc = test(model, test_loader)\n",
        "    print(f'Test Loss: {test_loss:.2f} | Test Acc: {test_acc*100:.2f}%')\n",
        "    \n",
        "    return model\n",
        "\n",
        "def test(model, loader):\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    model.eval()\n",
        "    loss = 0\n",
        "    acc = 0\n",
        "\n",
        "    for data in loader:\n",
        "      _, out = model(data.x, data.edge_index, data.batch)\n",
        "      loss += criterion(out, data.y) / len(loader)\n",
        "      acc += accuracy(out.argmax(dim=1), data.y) / len(loader)\n",
        "\n",
        "    return loss, acc\n",
        "\n",
        "def accuracy(pred_y, y):\n",
        "    \"\"\"Calculate accuracy.\"\"\"\n",
        "    return ((pred_y == y).sum() / len(y)).item()\n",
        "\n",
        "gcn = train(gcn, train_loader)\n",
        "gin = train(gin, train_loader)"
      ]
    }
  ]
}